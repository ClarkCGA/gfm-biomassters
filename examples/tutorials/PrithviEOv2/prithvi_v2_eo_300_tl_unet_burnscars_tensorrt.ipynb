{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W_4z81Fn9RET",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install terratorch==1.0.1 gdown tensorrt onnx onnxruntime polygraphy numpy pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "import warnings\n",
    "import time\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dw5-9A4A4OmI",
   "metadata": {
    "id": "dw5-9A4A4OmI",
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('hls_burn_scars.tar.gz'):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1yFDNlGqGPxkc9lh9l1O70TuejXAQYYtC\")\n",
    "\n",
    "if not os.path.isdir('hls_burn_scars/'):\n",
    "    !tar -xzvf hls_burn_scars.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3",
   "metadata": {
    "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3"
   },
   "outputs": [],
   "source": [
    "dataset_path = Path('hls_burn_scars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "outputs": [],
   "source": [
    "datamodule = terratorch.datamodules.GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=4,\n",
    "    num_workers=0,\n",
    "    num_classes=2,\n",
    "\n",
    "    # Define dataset paths \n",
    "    train_data_root=dataset_path / 'data/',\n",
    "    train_label_data_root=dataset_path / 'data/',\n",
    "    val_data_root=dataset_path / 'data/',\n",
    "    val_label_data_root=dataset_path / 'data/',\n",
    "    test_data_root=dataset_path / 'data/',\n",
    "    test_label_data_root=dataset_path / 'data/',\n",
    "\n",
    "    # Define splits\n",
    "    train_split=dataset_path / 'splits/train.txt',\n",
    "    val_split=dataset_path / 'splits/val.txt',\n",
    "    test_split=dataset_path / 'splits/test.txt',\n",
    "    \n",
    "    img_grep='*_merged.tif',\n",
    "    label_grep='*.mask.tif',\n",
    "    \n",
    "    train_transform=[\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "        \n",
    "    # Define standardization values\n",
    "    means=[\n",
    "      0.0333497067415863,\n",
    "      0.0570118552053618,\n",
    "      0.0588974813200132,\n",
    "      0.2323245113436119,\n",
    "      0.1972854853760658,\n",
    "      0.1194491422518656,\n",
    "    ],\n",
    "    stds=[\n",
    "      0.0226913556882377,\n",
    "      0.0268075602230702,\n",
    "      0.0400410984436278,\n",
    "      0.0779173242367269,\n",
    "      0.0870873883814014,\n",
    "      0.0724197947743781,\n",
    "    ],\n",
    "    no_data_replace=0,\n",
    "    no_label_replace=-1,\n",
    "    # We use all six bands of the data, so we don't need to define dataset_bands and output_bands.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "outputs": [],
   "source": [
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a30ddef8ed5a",
   "metadata": {},
   "source": [
    "# Export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"output/burnscars/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
    "    precision='bf16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True,  # Uses TensorBoard by default\n",
    "    max_epochs=1, # For demos\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"output/burnscars\",\n",
    "    detect_anomaly=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"prithvi_eo_v2_300\", # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 1, # 1 is the default value,\n",
    "        \"backbone_img_size\": 512,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        # \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11] # indices for prithvi_eo_v1_100\n",
    "                \"indices\": [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n",
    "                # \"indices\": [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n",
    "            },\n",
    "            {\"name\": \"ReshapeTokensToImage\",},\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"}            \n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 2,\n",
    "    },\n",
    "    \n",
    "    loss=\"ce\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True, # Only to speed up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    "    class_names=['no burned', 'burned']  # optionally define class names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeed29c-1ac5-4d6b-b524-d20c25959ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('hls_burn_scars.tar.gz'):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1-I_DiiO2T1mjBTi3OAJaVeRWKHtAG63N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a48c81-da6c-4f58-b3a1-5d63331ebfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint.pt\", map_location=torch.device('cuda')), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3359cd-9910-4a08-b29a-402d315aec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 6, 512, 512)\n",
    "original_forward = model.forward\n",
    "model.forward = lambda x: original_forward(x).output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2137644-c46f-454c-98ea-07387c8df9a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "core_model = model.model \n",
    "core_model.eval()\n",
    "#traced = torch.jit.trace(core_model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026209f-3f08-455b-aad4-02f3263b714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_forward = core_model.forward\n",
    "core_model.forward = lambda x: original_forward(x).output\n",
    "torch.onnx.export(\n",
    "    core_model,\n",
    "    dummy_input,\n",
    "    \"model2.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=17,  # or 13/17 depending on TRT support\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba31b3-89bc-4fbf-88bf-be1acf90698d",
   "metadata": {},
   "source": [
    "# Convert model from ONNX to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce99bd-d69f-47e7-a6d7-9d186b303842",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(flags=1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "with open(\"model2.onnx\", \"rb\") as f:\n",
    "    if not parser.parse(f.read()):\n",
    "        for i in range(parser.num_errors):\n",
    "            print(parser.get_error(i))\n",
    "        raise RuntimeError(\"ONNX parsing failed\")\n",
    "\n",
    "# Set optimization profile matching 5D input\n",
    "input_tensor = network.get_input(0)\n",
    "input_name = input_tensor.name\n",
    "profile = builder.create_optimization_profile()\n",
    "\n",
    "# Must be 5D shape: (batch_size, 6, 1, 224, 224)\n",
    "min_shape = (1, 6, 512, 512)\n",
    "opt_shape = (4, 6, 512, 512)\n",
    "max_shape = (4, 6, 512, 512)\n",
    "profile.set_shape(input_name, min=min_shape, opt=opt_shape, max=max_shape)\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "config.add_optimization_profile(profile)\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)\n",
    "\n",
    "# Build engine\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "if serialized_engine is None:\n",
    "    raise RuntimeError(\"Failed to build engine\")\n",
    "\n",
    "# Save engine\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(serialized_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5149dab-c18e-4194-aa9e-4a0ba244a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"checkpoint.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29dc02-d1b3-4d80-98ef-60e4584bad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_and_visual_inspection(model,ckpt_path):\n",
    "\n",
    "    # let's run the model on the test set\n",
    "    trainer.test(model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "        ckpt_path,\n",
    "        model_factory=model.hparams.model_factory,\n",
    "        model_args=model.hparams.model_args,\n",
    "    )    \n",
    "    end = time.time()\n",
    "    print(f'batch load time {end - start}')\n",
    "    # now we can use the model for predictions and plotting!\n",
    "\n",
    "    \n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        batch = next(iter(test_loader))\n",
    "        end = time.time()\n",
    "        print(f'batch load time {end - start}')\n",
    "        images = datamodule.aug(batch)\n",
    "        images = batch[\"image\"].to(model.device)\n",
    "        masks = batch[\"mask\"].numpy()\n",
    "\n",
    "        start = time.time()\n",
    "        outputs = model(images)\n",
    "        end = time.time()\n",
    "        print(f'interence time {end - start}')\n",
    "        preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "    \n",
    "    for i in range(4):\n",
    "        sample = {key: batch[key][i] for key in batch}\n",
    "        sample[\"prediction\"] = preds[i]\n",
    "        sample[\"image\"] = sample[\"image\"].cpu()\n",
    "        sample[\"mask\"] = sample[\"mask\"].cpu()\n",
    "        test_dataset.plot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af890844-88b1-4a33-a69c-95d2d7d3a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_and_visual_inspection(model, best_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047eb09-c5d6-4fd3-81b0-629977aa4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def run_test_and_visual_inspection_tensorrt():\n",
    "    import numpy as np\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.autoinit\n",
    "    import tensorrt as trt\n",
    "    import torch\n",
    "\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "\n",
    "    batch = next(iter(test_loader))\n",
    "    batch_size = 1  # Ensure batch size is 1 for TensorRT execution context\n",
    "\n",
    "    # Only use the first sample to keep it compatible with TensorRT engine (batch_size = 1)\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key][:batch_size]\n",
    "\n",
    "    # Apply any augmentation and get input image\n",
    "    batch = datamodule.aug(batch)\n",
    "    images = batch[\"image\"].to('cuda')  # Tensor on CUDA\n",
    "    masks = batch[\"mask\"].cpu().numpy()  # For visualization/comparison later\n",
    "\n",
    "    # Convert image tensor to NumPy and ensure float32 dtype\n",
    "    input_data = images.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # Define input/output shapes\n",
    "    input_shape = input_data.shape  # Should be (1, 6, 512, 512)\n",
    "    output_shape = (batch_size, 2, 512, 512)\n",
    "\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    start = time.time()\n",
    "    with open('model.trt', 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    end = time.time()\n",
    "    print(f'model load time: {end - start}')\n",
    "\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Allocate device memory\n",
    "    d_input = cuda.mem_alloc(int(np.prod(input_shape)) * np.float32().itemsize)\n",
    "    d_output = cuda.mem_alloc(int(np.prod(output_shape)) * np.float32().itemsize)\n",
    "\n",
    "    # Transfer input to device\n",
    "    cuda.memcpy_htod(d_input, input_data)\n",
    "\n",
    "    start = time.time()\n",
    "    context.execute_v2([int(d_input), int(d_output)])\n",
    "    end = time.time()\n",
    "    print(f'inference time: {end - start}')\n",
    "\n",
    "    # Get output from device\n",
    "    output_data = np.empty(output_shape, dtype=np.float32)\n",
    "    cuda.memcpy_dtoh(output_data, d_output)\n",
    "\n",
    "    print(\"Inference completed.\")\n",
    "    print(\"Output shape:\", output_data.shape)\n",
    "\n",
    "    # Post-process output\n",
    "    preds = torch.argmax(torch.from_numpy(output_data), dim=1).cpu().numpy()\n",
    "\n",
    "    # Visual inspection\n",
    "    for i in range(batch_size):\n",
    "        sample = {key: batch[key][i] for key in batch}\n",
    "        sample[\"prediction\"] = preds[i]\n",
    "        sample[\"image\"] = sample[\"image\"].cpu()\n",
    "        sample[\"mask\"] = sample[\"mask\"].cpu()\n",
    "        test_dataset.plot(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e627725-af3a-4369-88b8-2466c4187e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_and_visual_inspection_tensorrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5edb6-6dd1-41fc-b8a5-37a78f551c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n",
    "class ClassifierWrapper(LightningModule):\n",
    "    def __init__(self, base_model: LightningModule):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model.eval()\n",
    "        self.accuracy = BinaryAccuracy(ignore_index=-1)\n",
    "        self.precision = BinaryPrecision(ignore_index=-1)\n",
    "        self.recall = BinaryRecall(ignore_index=-1)\n",
    "        self.f1 = BinaryF1Score(ignore_index=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]  # (B, C, H, W)\n",
    "        y = batch[\"mask\"]   # (B, H, W)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            probs = output.output  # (B, 1, H, W)\n",
    "            preds = torch.argmax(probs, dim=1)  # (B, H, W) as torch.long\n",
    "\n",
    "        preds_flat = preds.reshape(-1)\n",
    "        y_flat = y.reshape(-1)\n",
    "        \n",
    "        self.accuracy(preds_flat, y_flat)\n",
    "        self.precision(preds_flat, y_flat)\n",
    "        self.recall(preds_flat, y_flat)\n",
    "        self.f1(preds_flat, y_flat)\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test/accuracy\", self.accuracy.compute(), prog_bar=True)\n",
    "        self.log(\"test/precision\", self.precision.compute(), prog_bar=True)\n",
    "        self.log(\"test/recall\", self.recall.compute(), prog_bar=True)\n",
    "        self.log(\"test/f1\", self.f1.compute(), prog_bar=True)\n",
    "\n",
    "        self.accuracy.reset()\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.f1.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899efeb5-f474-4d17-a503-342573b4db02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = ClassifierWrapper(model)\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", devices=1 if torch.cuda.is_available() else None)\n",
    "trainer.test(classifier, dataloaders=datamodule.test_dataloader())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
