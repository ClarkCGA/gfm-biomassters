{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bacc318390456b",
   "metadata": {},
   "source": [
    "# Setup\n",
    "1. In colab: Go to \"Runtime\" -> \"Change runtime type\" -> Select \"T4 GPU\"\n",
    "2. Install TerraTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W_4z81Fn9RET",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install terratorch==1.0.1 gdown tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b65b8e7cd7d65",
   "metadata": {},
   "source": [
    "3. Download the dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dw5-9A4A4OmI",
   "metadata": {
    "id": "dw5-9A4A4OmI",
    "jupyter": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('hls_burn_scars.tar.gz'):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1yFDNlGqGPxkc9lh9l1O70TuejXAQYYtC\")\n",
    "\n",
    "if not os.path.isdir('hls_burn_scars/'):\n",
    "    !tar -xzvf hls_burn_scars.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## HLS Burn Scars Dataset\n",
    "\n",
    "Lets start with analyzing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3",
   "metadata": {
    "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3"
   },
   "outputs": [],
   "source": [
    "dataset_path = Path('hls_burn_scars')\n",
    "!ls \"hls_burn_scars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84969a1f8bcae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"hls_burn_scars/data/\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "outputs": [],
   "source": [
    "datamodule = terratorch.datamodules.GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    num_classes=2,\n",
    "\n",
    "    # Define dataset paths \n",
    "    train_data_root=dataset_path / 'data/',\n",
    "    train_label_data_root=dataset_path / 'data/',\n",
    "    val_data_root=dataset_path / 'data/',\n",
    "    val_label_data_root=dataset_path / 'data/',\n",
    "    test_data_root=dataset_path / 'data/',\n",
    "    test_label_data_root=dataset_path / 'data/',\n",
    "\n",
    "    # Define splits\n",
    "    train_split=dataset_path / 'splits/train.txt',\n",
    "    val_split=dataset_path / 'splits/val.txt',\n",
    "    test_split=dataset_path / 'splits/test.txt',\n",
    "    \n",
    "    img_grep='*_merged.tif',\n",
    "    label_grep='*.mask.tif',\n",
    "    \n",
    "    train_transform=[\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "        \n",
    "    # Define standardization values\n",
    "    means=[\n",
    "      0.0333497067415863,\n",
    "      0.0570118552053618,\n",
    "      0.0588974813200132,\n",
    "      0.2323245113436119,\n",
    "      0.1972854853760658,\n",
    "      0.1194491422518656,\n",
    "    ],\n",
    "    stds=[\n",
    "      0.0226913556882377,\n",
    "      0.0268075602230702,\n",
    "      0.0400410984436278,\n",
    "      0.0779173242367269,\n",
    "      0.0870873883814014,\n",
    "      0.0724197947743781,\n",
    "    ],\n",
    "    no_data_replace=0,\n",
    "    no_label_replace=-1,\n",
    "    # We use all six bands of the data, so we don't need to define dataset_bands and output_bands.\n",
    ")\n",
    "\n",
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08644e71-d82f-426c-b0c1-79026fccb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking datasets train split size\n",
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking datasets validation split size\n",
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc29b1698dc4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a few samples\n",
    "val_dataset.plot(val_dataset[0])\n",
    "val_dataset.plot(val_dataset[6])\n",
    "val_dataset.plot(val_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "outputs": [],
   "source": [
    "# checking datasets testing split size\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a30ddef8ed5a",
   "metadata": {},
   "source": [
    "# Fine-tune Prithvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:root:Loaded weights for HLSBands.BLUE in position 0 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.GREEN in position 1 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.RED in position 2 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.NIR_NARROW in position 3 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_1 in position 4 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_2 in position 5 of patch embed\n",
      "WARNING:root:Decoder UNetDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"output/burnscars/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
    "    precision='bf16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True,  # Uses TensorBoard by default\n",
    "    max_epochs=1, # For demos\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"output/burnscars\",\n",
    "    detect_anomaly=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"prithvi_eo_v2_300\", # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 1, # 1 is the default value,\n",
    "        \"backbone_img_size\": 512,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        # \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11] # indices for prithvi_eo_v1_100\n",
    "                \"indices\": [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n",
    "                # \"indices\": [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n",
    "            },\n",
    "            {\"name\": \"ReshapeTokensToImage\",},\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"}            \n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 2,\n",
    "    },\n",
    "    \n",
    "    loss=\"ce\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True, # Only to speed up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    "    class_names=['no burned', 'burned']  # optionally define class names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0ec00057c56dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a48c81-da6c-4f58-b3a1-5d63331ebfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSegmentationTask(\n",
       "  (model): PixelWiseModel(\n",
       "    (encoder): PrithviViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(6, 1024, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): UNetDecoder(\n",
       "      (decoder): UnetDecoder(\n",
       "        (center): Identity()\n",
       "        (blocks): ModuleList(\n",
       "          (0): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention1): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (attention2): Attention(\n",
       "              (attention): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): SegmentationHead(\n",
       "      (head): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (aux_heads): ModuleDict()\n",
       "    (neck): Sequential(\n",
       "      (0): SelectIndices()\n",
       "      (1): ReshapeTokensToImage()\n",
       "      (2): LearnedInterpolateToPyramidal(\n",
       "        (fpn1): Sequential(\n",
       "          (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (fpn2): Sequential(\n",
       "          (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (fpn3): Sequential(\n",
       "          (0): Identity()\n",
       "        )\n",
       "        (fpn4): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (train_metrics): MetricCollection(\n",
       "    (Multiclass_Accuracy): MulticlassAccuracy()\n",
       "    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassAccuracy()\n",
       "    )\n",
       "    (Multiclass_F1_Score): MulticlassF1Score()\n",
       "    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
       "    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassJaccardIndex()\n",
       "    )\n",
       "    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
       "    prefix=train/\n",
       "  )\n",
       "  (val_metrics): MetricCollection(\n",
       "    (Multiclass_Accuracy): MulticlassAccuracy()\n",
       "    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassAccuracy()\n",
       "    )\n",
       "    (Multiclass_F1_Score): MulticlassF1Score()\n",
       "    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
       "    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassJaccardIndex()\n",
       "    )\n",
       "    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
       "    prefix=val/\n",
       "  )\n",
       "  (test_metrics): ModuleList(\n",
       "    (0): MetricCollection(\n",
       "      (Multiclass_Accuracy): MulticlassAccuracy()\n",
       "      (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
       "        (metric): MulticlassAccuracy()\n",
       "      )\n",
       "      (Multiclass_F1_Score): MulticlassF1Score()\n",
       "      (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
       "      (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
       "        (metric): MulticlassJaccardIndex()\n",
       "      )\n",
       "      (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
       "      prefix=test/\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load(\"checkpoint.pt\", map_location=torch.device('cpu')), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3359cd-9910-4a08-b29a-402d315aec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 6, 1, 224, 224)\n",
    "original_forward = model.forward\n",
    "model.forward = lambda x: original_forward(x).output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2137644-c46f-454c-98ea-07387c8df9a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelWiseModel(\n",
       "  (encoder): PrithviViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(6, 1024, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): UNetDecoder(\n",
       "    (decoder): UnetDecoder(\n",
       "      (center): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderBlock(\n",
       "          (conv1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention1): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "          (conv2): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (attention2): Attention(\n",
       "            (attention): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): SegmentationHead(\n",
       "    (head): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (aux_heads): ModuleDict()\n",
       "  (neck): Sequential(\n",
       "    (0): SelectIndices()\n",
       "    (1): ReshapeTokensToImage()\n",
       "    (2): LearnedInterpolateToPyramidal(\n",
       "      (fpn1): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (fpn2): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (fpn3): Sequential(\n",
       "        (0): Identity()\n",
       "      )\n",
       "      (fpn4): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_model = model.model \n",
    "core_model.eval()\n",
    "#traced = torch.jit.trace(core_model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d026209f-3f08-455b-aad4-02f3263b714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/utils.py:44: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if t_pad > 0:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/utils.py:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif h_pad > 0 or w_pad > 0:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/backbones/prithvi_mae.py:199: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if T / self.patch_size[0] % 1 or H / self.patch_size[1] % 1 or W / self.patch_size[2] % 1:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/backbones/prithvi_mae.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if [t_patches, h_patches, w_patches] == grid_size:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/backbones/prithvi_mae.py:149: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if t_patches != grid_size[0]:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/numpy/_core/fromnumeric.py:86: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/necks.py:167: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  h = int(math.sqrt(tokens_per_timestep))\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/pixel_wise_model.py:130: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.rescale and mask.shape[-2:] != input_size:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib64/python3.11/site-packages/terratorch/models/pixel_wise_model.py:93: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape[1] == 1:\n"
     ]
    }
   ],
   "source": [
    "original_forward = core_model.forward\n",
    "core_model.forward = lambda x: original_forward(x).output\n",
    "torch.onnx.export(\n",
    "    core_model,\n",
    "    dummy_input,\n",
    "    \"model.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=17,  # or 13/17 depending on TRT support\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429db628-6aec-4857-ab88-8d4083f5d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorrt onnx onnxruntime polygraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce99bd-d69f-47e7-a6d7-9d186b303842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "with open(\"model.onnx\", \"rb\") as f:\n",
    "    parser.parse(f.read())\n",
    "\n",
    "builder.max_workspace_size = 1 << 30  # 1GB\n",
    "engine = builder.build_cuda_engine(network)\n",
    "\n",
    "with open(\"model.trt\", \"wb\") as f:\n",
    "    f.write(engine.serialize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff284062edfce308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5149dab-c18e-4194-aa9e-4a0ba244a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"output/burnscars/checkpoints/best-epoch=00.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29dc02-d1b3-4d80-98ef-60e4584bad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_and_visual_inspection(model,ckpt_path):\n",
    "\n",
    "    # let's run the model on the test set\n",
    "    trainer.test(model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
    "\n",
    "    # now we can use the model for predictions and plotting!\n",
    "    model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "        ckpt_path,\n",
    "        model_factory=model.hparams.model_factory,\n",
    "        model_args=model.hparams.model_args,\n",
    "    )\n",
    "    \n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(test_loader))\n",
    "        images = datamodule.aug(batch)\n",
    "        images = batch[\"image\"].to(model.device)\n",
    "        masks = batch[\"mask\"].numpy()\n",
    "    \n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "    \n",
    "    for i in range(4):\n",
    "        sample = {key: batch[key][i] for key in batch}\n",
    "        sample[\"prediction\"] = preds[i]\n",
    "        sample[\"image\"] = sample[\"image\"].cpu()\n",
    "        sample[\"mask\"] = sample[\"mask\"].cpu()\n",
    "        test_dataset.plot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af890844-88b1-4a33-a69c-95d2d7d3a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_and_visual_inspection(model, best_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c41c8-16ce-499e-a2ab-25cfcd5a6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_100_epoch_path = \"burnscars_best-epoch=69.ckpt\"\n",
    "\n",
    "if not os.path.isfile(best_ckpt_100_epoch_path):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1-I_DiiO2T1mjBTi3OAJaVeRWKHtAG63N\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdaa496-8359-4acb-ab4a-836e667d026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test_and_visual_inspection(model, best_ckpt_100_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54779ae510fefc8f",
   "metadata": {},
   "source": [
    "# Fine-tuning via CLI\n",
    "\n",
    "You might want to restart the session to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8e91384bb8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fine-tuning\n",
    "!terratorch fit -c prithvi_v2_eo_300_tl_unet_burnscars.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac095bd-86ab-4a81-8ad9-d6acd4f4577d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
