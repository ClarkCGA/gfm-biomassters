{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff0785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchgeo\n",
    "import terratorch\n",
    "import yaml\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b04cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torchgeo.trainers import ObjectDetectionTask\n",
    "# from terratorch.tasks import ObjectDetectionTask\n",
    "from torchgeo.datasets import VHR10\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1860bcd-87a8-4348-9102-7ed758c38360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from collections.abc import Callable\n",
    "from functools import partial\n",
    "from albumentations.pytorch import transforms as T\n",
    "import numpy as np\n",
    "\n",
    "class Normalize(Callable):\n",
    "    def __init__(self, means, stds, max_pixel_value=None):\n",
    "        super().__init__()\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "    def __call__(self,  **kwargs):\n",
    "        # min_value = self.means - 2 * self.stds\n",
    "        # max_value = self.means + 2 * self.stds\n",
    "        # img = (batch[\"image\"] - min_value) / (max_value - min_value)\n",
    "        # img = torch.clip(img, 0, 1)\n",
    "        # batch[\"image\"] = img\n",
    "        # return batch\n",
    "        pdb.set_trace()\n",
    "        batch = kwargs\n",
    "        batch['image']=torch.stack(tuple(batch[\"image\"]))\n",
    "        image = batch[\"image\"]/max_pixel_value if max_pixel_value is not None else batch[\"image\"]\n",
    "        if len(image.shape) == 5:\n",
    "            means = torch.tensor(self.means, device=image.device).view(1, -1, 1, 1, 1)\n",
    "            stds = torch.tensor(self.stds, device=image.device).view(1, -1, 1, 1, 1)\n",
    "        elif len(image.shape) == 4:\n",
    "            means = torch.tensor(self.means, device=image.device).view(1, -1, 1, 1)\n",
    "            stds = torch.tensor(self.stds, device=image.device).view(1, -1, 1, 1)\n",
    "        else:\n",
    "            msg = f\"Expected batch to have 5 or 4 dimensions, but got {len(image.shape)}\"\n",
    "            raise Exception(msg)\n",
    "        batch[\"image\"] = (image - means) / stds\n",
    "        # pdb.set_trace()\n",
    "        return batch\n",
    "\n",
    "\n",
    "def get_transform(train, image_size=1472):\n",
    "    transforms = []\n",
    "    transforms.append(A.PadIfNeeded(min_height=image_size, min_width=image_size, value=0, border_mode=0))\n",
    "    if train:\n",
    "        transforms.append(A.CenterCrop(width=image_size, height=image_size))\n",
    "        transforms.append(A.HorizontalFlip(p=0.5))\n",
    "    else:\n",
    "        transforms.append(A.CenterCrop(width=image_size, height=image_size))\n",
    "    transforms.append(A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255))\n",
    "    transforms.append(A.ToFloat())\n",
    "    transforms.append(T.ToTensorV2())\n",
    "    # return A.Compose(transforms, additional_targets={'boxes': 'bboxes', 'masks': 'mask'})\n",
    "    return A.Compose(transforms, bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=['labels']), is_check_shapes=False)\n",
    "\n",
    "def apply_transforms(sample, transforms):\n",
    "\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    sample['image']=torch.stack(tuple(sample[\"image\"]))\n",
    "    sample['image'] = sample['image'].permute(1, 2, 0) if len(sample['image'].shape) == 3 else sample['image'].permute(0, 2, 3, 1)\n",
    "    sample['image'] = np.array(sample['image'].cpu())\n",
    "    sample[\"masks\"] = [np.array(torch.stack(tuple(x)).cpu()) for x in sample[\"masks\"]]\n",
    "    # sample[\"masks\"] = np.array(sample[\"masks\"].cpu())\n",
    "    sample[\"boxes\"] = np.array(sample[\"boxes\"].cpu())\n",
    "    sample[\"labels\"] = np.array(sample[\"labels\"].cpu())\n",
    "    # sample[\"boxes\"] = [torch.stack(tuple(x)) for x in sample[\"masks\"]]\n",
    "    # sample[\"labels\"] =  \n",
    "    transformed = transforms(image=sample['image'],\n",
    "                             masks=sample[\"masks\"], \n",
    "                             # bboxes=np.array(torch.stack(tuple(sample[\"boxes\"]), dim=0).cpu()), \n",
    "                             bboxes=sample[\"boxes\"],\n",
    "                             labels=sample[\"labels\"])\n",
    "    \n",
    "    transformed['boxes'] = torch.tensor(transformed['bboxes'])\n",
    "    transformed['labels'] = torch.tensor(transformed['labels'], dtype=torch.int64)\n",
    "    masks_before = len(transformed['masks'])\n",
    "    transformed['masks'] = [x for x in transformed['masks'] if x.any()]\n",
    "    masks_after = len(transformed['masks'])\n",
    "    if masks_before != masks_after:\n",
    "        print(\"Dropped masks:\", str(masks_before - masks_after))\n",
    "    del transformed['bboxes']\n",
    "    # print(\"Done transform\")\n",
    "    return transformed\n",
    "\n",
    "transforms = get_transform(True, image_size=896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95022f66-8513-4b4b-bcdc-c79a368b7384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from terratorch.datasets import mVHR10 as mVHR10Dataset\n",
    "from terratorch.datamodules import mVHR10DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a565672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    sample[\"image\"] = sample[\"image\"].float() / 255.0\n",
    "    return sample\n",
    "# ds = VHR10(root='data/VHR10/', split='positive', transforms=preprocess, download=True)\n",
    "ds_old = VHR10(root='data/VHR10/', split='positive', transforms=partial(apply_transforms, transforms=transforms), download=True)\n",
    "ds = mVHR10Dataset(second_level_split = 'test', root = 'data/VHR10/', transforms=partial(apply_transforms, transforms=transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69631d-ad1e-4345-942c-48b716898570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = mVHR10Dataset(second_level_split = 'train', root = 'data/VHR10/', transforms=partial(apply_transforms, transforms=transforms), second_level_split_proportions=(1, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b2039-86d5-435e-b04f-e3ccf24628bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_old = VHR10(root='data/VHR10/', split='positive', download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3540bb62-3012-4801-a216-5ccf9a35b670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_faulty = pd.DataFrame()\n",
    "\n",
    "# ds_old = VHR10(root='data/VHR10/', split='positive', download=True)\n",
    "\n",
    "# for i in range(len(ds)):\n",
    "    \n",
    "#     masks_n = len(ds[i]['masks'])\n",
    "#     boxes_n = len(ds[i]['boxes'])\n",
    "#     labels_n = len(ds[i]['labels'])\n",
    "    \n",
    "#     if ((masks_n != boxes_n) | (labels_n != boxes_n) | (labels_n != masks_n)):\n",
    "#         df_tmp = pd.DataFrame({'id': [i], 'masks_n': [masks_n], 'boxes_n': [boxes_n], 'labels_n': [labels_n]})\n",
    "#         df_faulty = pd.concat([df_faulty, df_tmp], axis=0)\n",
    "        \n",
    "for i in range(len(ds)):\n",
    "    \n",
    "    masks_n = len(ds[i]['masks'])\n",
    "    boxes_n = len(ds[i]['boxes'])\n",
    "    labels_n = len(ds[i]['labels'])\n",
    "    print(i)\n",
    "    if ((masks_n != boxes_n) | (labels_n != boxes_n) | (labels_n != masks_n)):\n",
    "        print(i)\n",
    "        df_tmp = pd.DataFrame({'id': [i], 'masks_n': [masks_n], 'boxes_n': [boxes_n], 'labels_n': [labels_n]})\n",
    "        df_faulty = pd.concat([df_faulty, df_tmp], axis=0)\n",
    "    print(\"#############\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b60404-27cc-45ef-988d-a752b3cc944e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdf6f0-467b-4205-a69e-5710343a1ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.plot(ds[85])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec889c-9631-4acd-a4fa-ce82b593d9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    ds.plot(ds_old[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b197d4-b18f-4d46-a6ea-b9757d389730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"image\": [item[\"image\"] for item in batch],\n",
    "        \"boxes\": [item[\"boxes\"] for item in batch],\n",
    "        \"labels\": [item[\"labels\"] for item in batch],\n",
    "        \"masks\": [item[\"masks\"] for item in batch],\n",
    "    }\n",
    "    return new_batch\n",
    "\n",
    "# dl = DataLoader(ds_old, batch_size=16, num_workers=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def4398-71c2-441c-b415-3b09b3485390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The current (torchgeo 0.5.0) ObjectDetectionTask does not seem to support variable\n",
    "# size inputs. We can quickly fix this by subclassing it and overriding the\n",
    "# training_step method.\n",
    "# from torchgeo.trainers import ObjectDetectionTask\n",
    "from terratorch.tasks import ObjectDetectionTask\n",
    "\n",
    "class VariableSizeInputObjectDetectionTask(ObjectDetectionTask):\n",
    "    def training_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        x = batch[\"image\"]\n",
    "        batch_size = len(x)  # we change this line to support variable size inputs\n",
    "        y = [\n",
    "            {\"boxes\": batch[\"boxes\"][i], \"labels\": batch[\"labels\"][i]}\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        loss_dict = self(x, y)\n",
    "        # pdb.set_trace()\n",
    "        if isinstance(loss_dict, dict) == False:\n",
    "            loss_dict = loss_dict.output\n",
    "        train_loss: Tensor = sum(loss_dict.values())\n",
    "        self.log(\"train_loss\", train_loss)\n",
    "        if torch.isnan(train_loss):\n",
    "            pdb.set_trace()\n",
    "        #self.log_dict(loss_dict)\n",
    "        return train_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c980dc-e994-42a6-a475-5b8bac78391a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# task = VariableSizeInputObjectDetectionTask(\n",
    "#     model=\"fcos\",\n",
    "#     backbone=\"resnet18\",\n",
    "#     weights=True,\n",
    "#     in_channels=3,\n",
    "#     num_classes=11,\n",
    "#     trainable_layers=3,\n",
    "#     lr=1e-3,\n",
    "#     patience=10,\n",
    "#     freeze_backbone=False,\n",
    "# )\n",
    "\n",
    "# tb_logger = TensorBoardLogger(save_dir=\"../../../logs/\", name=\"torchgeo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75517ac-af92-4166-ba41-b10d3350fc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open('../confs/object_detection_vhr10_prithvi.yaml', 'r') as file:\n",
    "with open('../confs/object_detection_vhr10_resnet50.yaml', 'r') as file:\n",
    "# with open('../confs/object_detection_vhr10_vit.yaml', 'r') as file:    \n",
    "#with open('../confs/object_detection_vhr10_Clay.yaml', 'r') as file:    \n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# framework = 'faster-rcnn' \n",
    "framework = 'mask-rcnn'\n",
    "# framework = 'fcos'\n",
    "# framework = 'retinanet'\n",
    "\n",
    "config['model']['init_args']['model_args']['framework'] = framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2b5b6-bd76-41a7-b1e0-39f261979751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['model']['init_args']['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f720e4-6b82-458d-b834-677ea86410fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e04c3-de81-4f9b-9645-887c1824b46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = ObjectDetectionTask(\n",
    "    optimizer=config['optimizer']['class_path'].split('.')[-1],\n",
    "    optimizer_hparams=config['optimizer']['init_args'],\n",
    "    scheduler=config['lr_scheduler']['class_path'].split('.')[-1],\n",
    "    scheduler_hparams=config['lr_scheduler']['init_args'],\n",
    "    # patience=10,\n",
    "    **config['model']['init_args']\n",
    ")\n",
    "\n",
    "# tb_logger = TensorBoardLogger(save_dir=\"../../../logs/\", name=config['trainer']['logger']['init_args']['name'])\n",
    "tb_logger = TensorBoardLogger(save_dir=\"../../../logs/\", name=framework + '_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d2a11-4986-4203-b54a-5612da5d6a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcf8a0-9a3a-4ef5-a94b-2a30745850ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task.monitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210453c-9f32-4ccd-863b-cff0e18d3f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# task.monitor = \"loss_classifier\"\n",
    "# task.monitor = \"train_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ac66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"logs/\",\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    # accelerator=\"cpu\",\n",
    "    min_epochs=0,\n",
    "    max_epochs=2,\n",
    "    log_every_n_steps=20,\n",
    "    logger=tb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebd747-3522-4910-aa88-8daa11f5c77c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"GPU available: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba5fb9-b41d-4371-bbbb-2f760478f2a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datamodule = mVHR10DataModule(root='data/VHR10/', batch_size=8, second_level_split_proportions=(0.9, 0.05, 0.05))\n",
    "datamodule = mVHR10DataModule(root='data/VHR10/', batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e339b-787e-4d40-b9b3-6f0cf97434ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483eee82-3950-4a46-8060-1da49a10b9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c132e",
   "metadata": {},
   "source": [
    "## Inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f06fbd-7bca-4dfb-8791-c9e1cca3c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/opt/app-root/src/fm-geospatial/pf/logs/terratorch_datamodule/version_7/checkpoints/epoch=28-step=2146.ckpt\"\n",
    "path = \"/opt/app-root/src/fm-geospatial/pf/logs/terratorch/version_9/checkpoints/epoch=99-step=4100.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de1237-9d03-475d-9219-8c5e87e79ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53224cfa-791d-491c-a0c8-9c946650afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state_dict['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd179b13-8f45-44ca-88fa-598c77e43b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "for key in state_dict.keys():\n",
    "    new_key = key.replace('model.torchvision', 'torchvision')  # Replace with your specific changes\n",
    "    new_state_dict[new_key] = state_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d3696-62fc-4c95-a78c-f26a714660cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534f417-17f4-4c56-925c-5b97b5ac65e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e432bcb-325b-44df-b975-dde61fb2e5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = task.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdccdff-f0d7-44c5-84d6-dc783005a371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0276ac2-5004-4eeb-9f9c-7833058a5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fa405-9195-4a6c-a3c1-b5ea9938695d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd77155-4976-44f3-9782-d73d9291c0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import nms\n",
    "\n",
    "def apply_nms(boxes, scores, labels, iou_threshold=0.1, score_threshold=0.1):\n",
    "    # Filter based on score threshold\n",
    "    keep_score = scores > score_threshold\n",
    "    boxes, scores, labels = boxes[keep_score], scores[keep_score], labels[keep_score]\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep_nms = nms(boxes, scores, iou_threshold)\n",
    "    \n",
    "    return boxes[keep_nms], scores[keep_nms], labels[keep_nms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd303557-00c5-48ac-97c2-03956f1e8b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datamodule.setup('test')\n",
    "datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f2988-3c8e-407c-aae8-3d65de611092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b72426-bf83-4f5d-8e61-c9f41230e3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dl = datamodule._dataloader_factory('test')\n",
    "# train_dl = datamodule._dataloader_factory('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758f62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n",
    "# batch = next(iter(train_dl))\n",
    "# batch = next(iter(dl))\n",
    "#batch[\"image\"] = [image.to(\"cuda:0\") for image in batch[\"image\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c27f0-33c7-43ef-a7d2-271f1ae752ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = datamodule.aug(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb11e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(batch[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f82add-f13b-44eb-b883-41edc721e692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    output_list = out.output\n",
    "except:\n",
    "    print(\"No wrapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ed571-ef58-452f-8903-096cc2e687cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, out in enumerate(output_list):\n",
    "    print(str(i), '   ###################################')\n",
    "    print(out)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b91d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch_idx in range(len(batch[\"image\"])):\n",
    "\n",
    "    sample = {\n",
    "        \"image\": batch[\"image\"][batch_idx],\n",
    "        \"boxes\": batch[\"boxes\"][batch_idx],\n",
    "        \"labels\": batch[\"labels\"][batch_idx],\n",
    "        \"masks\": batch[\"masks\"][batch_idx],\n",
    "        \"prediction_labels\":  output_list[batch_idx][\"labels\"],\n",
    "        \"prediction_boxes\":  output_list[batch_idx][\"boxes\"],\n",
    "        \"prediction_scores\": output_list[batch_idx][\"scores\"],\n",
    "        \"prediction_masks\": output_list[batch_idx][\"masks\"],\n",
    "    }\n",
    "    ds.plot(sample)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# sample[\"prediction_boxes\"], sample[\"prediction_scores\"],sample[\"prediction_labels\"]  = apply_nms(sample[\"prediction_boxes\"], sample[\"prediction_scores\"],sample[\"prediction_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74f6e1-e8ad-4b00-a53c-bbe94f0be013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.plot(sample)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
