{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d049232-f4b1-473d-aac3-0b3539905b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import terratorch # even though we don't use the import directly, we need it so that the models are available in the timm registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83560911-91ed-4586-b4ec-9a3bcd80fce4",
   "metadata": {},
   "source": [
    "# Backbone factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b84f9-3846-4c82-b0dc-d96052b81325",
   "metadata": {},
   "source": [
    "Learn more about timm at https://huggingface.co/docs/timm/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcdfa85-8e43-4db0-9ddf-cb11c5544942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prithvi_swin_B', 'prithvi_swin_B', 'prithvi_swin_B_MP', 'prithvi_swin_L', 'prithvi_swin_L_MP', 'prithvi_vit_100', 'prithvi_vit_100_us', 'prithvi_vit_300', 'prithvi_vit_tiny']\n",
      "['prithvi_swin_B', 'prithvi_swin_B', 'prithvi_swin_L', 'prithvi_vit_100', 'prithvi_vit_100_us', 'prithvi_vit_300']\n"
     ]
    }
   ],
   "source": [
    "# find available prithvi models by name\n",
    "print(timm.list_models(\"prithvi*\"))\n",
    "# and those with pretrained weights\n",
    "print(timm.list_pretrained(\"prithvi*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13134d11-c477-47c2-998d-a9acb084e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your desired model with features_only=True to obtain a backbone\n",
    "# this defaults to the weights present in CCC.\n",
    "model = timm.create_model(\n",
    "    \"prithvi_swin_B\", num_frames=1, pretrained=True, features_only=True\n",
    ")\n",
    "\n",
    "# Rest of your PyTorch / PyTorchLightning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b937c-cbab-4e34-a113-874fa9bac983",
   "metadata": {},
   "source": [
    "## The model\n",
    "The resulting model is a torch module. Because we set `features_only=True`, it is only the encoder portion.\n",
    "By default, the model is instantiated with the same bands it was pretrained on, with the same order.\n",
    "\n",
    "We can inspect both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1040a09-fa6c-40e2-9a6f-8a3a60c520b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was pretrained on bands [<HLSBands.BLUE: 'BLUE'>, <HLSBands.GREEN: 'GREEN'>, <HLSBands.RED: 'RED'>, <HLSBands.NIR_NARROW: 'NIR_NARROW'>, <HLSBands.SWIR_1: 'SWIR_1'>, <HLSBands.SWIR_2: 'SWIR_2'>].\n",
      " The model is using bands [<HLSBands.BLUE: 'BLUE'>, <HLSBands.GREEN: 'GREEN'>, <HLSBands.RED: 'RED'>, <HLSBands.NIR_NARROW: 'NIR_NARROW'>, <HLSBands.SWIR_1: 'SWIR_1'>, <HLSBands.SWIR_2: 'SWIR_2'>]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model was pretrained on bands {model.pretrained_bands}.\\n The model is using bands {model.model_bands}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb25d1f-ad67-4b11-b6bd-fdfcd0ad32e5",
   "metadata": {},
   "source": [
    "The model output is a list with the output of each encoder stage. This may be different for each encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f19710-1bec-4cb1-a5ec-d9a51556dd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index 0 has shape torch.Size([1, 56, 56, 128])\n",
      "Feature index 1 has shape torch.Size([1, 28, 28, 256])\n",
      "Feature index 2 has shape torch.Size([1, 14, 14, 512])\n",
      "Feature index 3 has shape torch.Size([1, 7, 7, 1024])\n"
     ]
    }
   ],
   "source": [
    "trial_data = torch.zeros(1, 6, 224, 224) # batch_size, channels, height, width\n",
    "features = model(trial_data)\n",
    "for index, feature in enumerate(features):\n",
    "    print(f\"Feature index {index} has shape {feature.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b560ae-88a6-4e8a-8a9d-a88ea8c5aaef",
   "metadata": {},
   "source": [
    "## Band choice\n",
    "Sometimes you may wish to use a separate set of bands than was used in pretraining. This may be a different ordering, a subset, a superset, or a completely different set.\n",
    "\n",
    "To do this, you may specify the bands you wish to train on using a mixture of integers and members of the `HLSBands` enum.\n",
    "\n",
    "In the patch embed layer, the weights corresponding to bands that exist in the pretrained bands will be mapped to the correct order. Bands that do not exist will be randomly initialized.\n",
    "\n",
    "**Warning:** the enum maps to integers 1 through 12. If using integers, make sure they are outside this range!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48e98bf-c748-47c6-b56f-98c96304ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datasets import HLSBands\n",
    "# lets get only the RGB bands, and put them in that order rather than BGR, and lets add an extra band not in HLSBands\n",
    "bands = [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE, 14]\n",
    "model = timm.create_model( # let's use a vit model this time\n",
    "    \"prithvi_vit_100\", num_frames=1, pretrained=True, features_only=True, bands=bands\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5936e883-d6c0-470a-95e0-51fc121274ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<HLSBands.RED: 'RED'>, <HLSBands.GREEN: 'GREEN'>, <HLSBands.BLUE: 'BLUE'>, 14]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b850f30d-eebe-4da2-8bf0-f58e3fd2ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index 0 has shape torch.Size([1, 197, 768])\n",
      "Feature index 1 has shape torch.Size([1, 197, 768])\n",
      "Feature index 2 has shape torch.Size([1, 197, 768])\n",
      "Feature index 3 has shape torch.Size([1, 197, 768])\n",
      "Feature index 4 has shape torch.Size([1, 197, 768])\n",
      "Feature index 5 has shape torch.Size([1, 197, 768])\n",
      "Feature index 6 has shape torch.Size([1, 197, 768])\n",
      "Feature index 7 has shape torch.Size([1, 197, 768])\n",
      "Feature index 8 has shape torch.Size([1, 197, 768])\n",
      "Feature index 9 has shape torch.Size([1, 197, 768])\n",
      "Feature index 10 has shape torch.Size([1, 197, 768])\n",
      "Feature index 11 has shape torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# the model now expects 4 channels, not 6\n",
    "trial_data = torch.zeros(1, 4, 224, 224) # batch_size, channels, height, width\n",
    "features = model(trial_data)\n",
    "for index, feature in enumerate(features):\n",
    "    print(f\"Feature index {index} has shape {feature.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961690ed-32ec-4e51-8b56-8dff89efed62",
   "metadata": {},
   "source": [
    "# Model factory\n",
    "The model factories let us create full models ready for specific tasks, including decoders and task specific heads.\n",
    "They create normal `torch.nn.Module` s that you can use anywhere in your code.\n",
    "\n",
    "Lets create a model for semantic segmentation with 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b8503c7-22c5-48b6-9edc-ee80d83b2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.models import PrithviModelFactory\n",
    "model_factory = PrithviModelFactory()\n",
    "\n",
    "# Let's build a segmentation model\n",
    "# Parameters prefixed with backbone_ get passed to the backbone\n",
    "# Parameters prefixed with decoder_ get passed to the decoder\n",
    "# Parameters prefixed with head_ get passed to the head\n",
    "\n",
    "model = model_factory.build_model(task=\"segmentation\",\n",
    "        backbone=\"prithvi_vit_100\",\n",
    "        decoder=\"FCNDecoder\",\n",
    "        in_channels=6,\n",
    "        bands=[\n",
    "            HLSBands.BLUE,\n",
    "            HLSBands.GREEN,\n",
    "            HLSBands.RED,\n",
    "            HLSBands.NIR_NARROW,\n",
    "            HLSBands.SWIR_1,\n",
    "            HLSBands.SWIR_2,\n",
    "        ],\n",
    "        num_classes=4,\n",
    "        pretrained=True,\n",
    "        num_frames=1,\n",
    "        decoder_channels=128,\n",
    "        head_dropout=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ec4cf-0dbc-4fe4-a0fd-03973710b6fa",
   "metadata": {},
   "source": [
    "Their output is a `ModelOutput` object, including the main `output` and the output of any `auxiliary_heads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f432377b-189e-4268-80ea-d9eea721123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "trial_data = torch.zeros(1, 6, 224, 224) # batch_size, channels, height, width\n",
    "out = model(trial_data)\n",
    "print(out.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366995b-2e37-4155-9a26-1d0ad842a60c",
   "metadata": {},
   "source": [
    "# Datamodule\n",
    "You can create datamodules for training by creating your own subclasses of `torchgeo.datamodules.GeoDataModule` or `torchgeo.datamodules.NonGeoDataModule`.\n",
    "\n",
    "Alternatively, leverage one of our generic data modules.\n",
    "\n",
    "Datamodules package train, test and validation datasets as well as any transforms done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a2e286-e5b2-4cf4-8aa7-44e874929117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datamodules import GenericNonGeoPixelwiseRegressionDataModule\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "train_val_test = [\n",
    "    \"/dccstor/hhr-weather/latest_filters_all_agb_patches_tts_clipped_0_500/train_images\",\n",
    "    \"/dccstor/hhr-weather/latest_filters_all_agb_patches_tts_clipped_0_500/val_images\",\n",
    "    \"/dccstor/hhr-weather/latest_filters_all_agb_patches_tts_clipped_0_500/test_images\",\n",
    "]\n",
    "\n",
    "train_val_test_labels = {\n",
    "    \"train_label_data_root\": \"/dccstor/hhr-weather/latest_filters_all_agb_patches_tts_clipped_0_500/train_labels\",\n",
    "    \"val_label_data_root\": \"/dccstor/hhr-weather/latest_filters_all_agb_patches_tts_clipped_0_500/val_labels\",\n",
    "    \"test_label_data_root\": \"/dccstor/hhr-weather/latest_filters_all_agb_patches_tts_clipped_0_500/test_labels\",\n",
    "}\n",
    "\n",
    "means = [385.88501817, 714.60615207, 658.96267376, 3314.57774238, 2238.71812558, 1250.00982518]\n",
    "stds = [264.62872, 355.62848, 504.54855, 898.4953, 947.22894, 828.1297]\n",
    "datamodule = GenericNonGeoPixelwiseRegressionDataModule(\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    *train_val_test,\n",
    "    means,\n",
    "    stds,\n",
    "    **train_val_test_labels,\n",
    "    # train_transform=train_transform,\n",
    "    dataset_bands=[\n",
    "        -1,\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2,\n",
    "        14,\n",
    "        15,\n",
    "        16,\n",
    "        17,\n",
    "    ],\n",
    "    output_bands=[\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2,\n",
    "    ],\n",
    ")\n",
    "# we want to access some properties of the train dataset later on, so lets call setup here\n",
    "# if not, we would not need to\n",
    "datamodule.setup(\"fit\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b6112-d818-48ea-86d6-8095cd3dd93b",
   "metadata": {},
   "source": [
    "# Lightning Trainers\n",
    "At the highest level of abstraction, you can operate with task specific trainers. These encapsulate the model, loss, optimizer and any training hyperparameters.\n",
    "\n",
    "They build on the model factory we introduced previously and are able to take any. To use a task with a model not supported by a currently existing model factory, simply create your own model factory!\n",
    "\n",
    "Let's create a Trainer for PixelWise Regression\n",
    "\n",
    "We also show how to use the popular CosineLrDecay scheduler into training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd3f466-5ee4-4897-8e1e-1cfafe6c4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.tasks import IBMPixelwiseRegressionTask\n",
    "from terratorch.models.model import AuxiliaryHead\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import math\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "model_args = {\n",
    "        \"backbone\":\"prithvi_vit_100\",\n",
    "        \"decoder\":\"FCNDecoder\",\n",
    "        \"in_channels\": 6,\n",
    "        \"bands\": [\n",
    "            HLSBands.RED,\n",
    "            HLSBands.GREEN,\n",
    "            HLSBands.BLUE,\n",
    "            HLSBands.NIR_NARROW,\n",
    "            HLSBands.SWIR_1,\n",
    "            HLSBands.SWIR_2,\n",
    "        ],\n",
    "        \"pretrained\": True,\n",
    "        \"num_frames\":1,\n",
    "        \"decoder_channels\":128,\n",
    "        \"head_dropout\":0.2\n",
    "}\n",
    "\n",
    "task = IBMPixelwiseRegressionTask(\n",
    "    model_args,\n",
    "    \"PrithviModelFactory\",\n",
    "    loss=\"rmse\",\n",
    "    aux_loss={\"fcn_aux_head\": 0.4},\n",
    "    lr=lr,\n",
    "    ignore_index=-1,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer_hparams={\"weight_decay\": 0.05},\n",
    "    scheduler=OneCycleLR,\n",
    "    scheduler_hparams={\n",
    "        \"max_lr\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"steps_per_epoch\": math.ceil(len(datamodule.train_dataset) / batch_size),\n",
    "        \"pct_start\": 0.05,\n",
    "        \"interval\": \"step\",\n",
    "    },\n",
    "    aux_heads=[\n",
    "        AuxiliaryHead( # define an auxiliary head\n",
    "            \"fcn_aux_head\",\n",
    "            \"FCNDecoder\",\n",
    "            {\"decoder_channels\": 512, \"decoder_in_index\": 2, \"decoder_num_convs\": 2, \"head_channel_list\": [64]},\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6a6a7-4613-472e-9f70-fbe28e0e4540",
   "metadata": {},
   "source": [
    "Now we can use a Lightning Trainer to train this model on the datamodule we specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4db99b3-72bb-46ba-b149-49493529d714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/0 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1502/1502</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:45 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">14.31it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 1.000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mEpoch 0/0 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m1502/1502\u001b[0m \u001b[38;5;245m0:01:45 • 0:00:00\u001b[0m \u001b[38;5;249m14.31it/s\u001b[0m \u001b[37mv_num: 1.000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "accelerator = \"gpu\"\n",
    "experiment = \"tutorial\"\n",
    "default_root_dir = os.path.join(\"/dccstor/geofm-finetuning/carlosgomes/\", \"tutorial_experiments\", experiment)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=task.monitor, save_top_k=1, save_last=True)\n",
    "early_stopping_callback = EarlyStopping(monitor=task.monitor, min_delta=0.00, patience=20)\n",
    "logger = TensorBoardLogger(save_dir=default_root_dir, name=experiment)\n",
    "\n",
    "trainer = Trainer(\n",
    "    # precision=\"16-mixed\",\n",
    "    accelerator=accelerator,\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        checkpoint_callback,\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ],\n",
    "    logger=logger,\n",
    "    max_epochs=1, # train only one epoch for demo\n",
    "    default_root_dir=default_root_dir,\n",
    ")\n",
    "trainer.fit(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e977b153-e9e8-4ef0-a3ff-714ff117a94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">187/187</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:14 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">12.77it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m187/187\u001b[0m \u001b[38;5;245m0:00:14 • 0:00:00\u001b[0m \u001b[38;5;249m12.77it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/MAE          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     74.75745391845703     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/MSE          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     13011.7548828125      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/RMSE         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    114.06907653808594     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/decode_head_epoch   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     95.1190185546875      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/fcn_aux_head_epoch  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     87.39353942871094     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     130.076416015625      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/MAE         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    74.75745391845703    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/MSE         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    13011.7548828125     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/RMSE        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   114.06907653808594    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/decode_head_epoch  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    95.1190185546875     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/fcn_aux_head_epoch \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    87.39353942871094    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    130.076416015625     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/loss': 130.076416015625,\n",
       "  'test/decode_head_epoch': 95.1190185546875,\n",
       "  'test/fcn_aux_head_epoch': 87.39353942871094,\n",
       "  'test/MAE': 74.75745391845703,\n",
       "  'test/MSE': 13011.7548828125,\n",
       "  'test/RMSE': 114.06907653808594}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e22c1-d5f3-4910-9be7-76eea0610ec5",
   "metadata": {},
   "source": [
    "# Configs\n",
    "Alternatively, define all this in a config file and run it through the cli. This is the way models must be specified to be onboarded to the studio.\n",
    "\n",
    "For example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0deeca3-e4c7-44f7-9de3-36269ed28ef9",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# lightning.pytorch==2.1.1\n",
    "seed_everything: 0\n",
    "trainer:\n",
    "  accelerator: auto\n",
    "  strategy: auto\n",
    "  devices: auto\n",
    "  num_nodes: 1\n",
    "  precision: 16-mixed\n",
    "  logger: True # will use tensorboardlogger\n",
    "  callbacks:\n",
    "    - class_path: RichProgressBar\n",
    "    - class_path: LearningRateMonitor\n",
    "      init_args:\n",
    "        logging_interval: epoch\n",
    "\n",
    "  max_epochs: 200\n",
    "  check_val_every_n_epoch: 1\n",
    "  log_every_n_steps: 50\n",
    "  enable_checkpointing: true\n",
    "  default_root_dir: <path to root dir>\n",
    "data:\n",
    "  class_path: GenericNonGeoSegmentationDataModule\n",
    "  init_args:\n",
    "    batch_size: 4\n",
    "    num_workers: 8\n",
    "    constant_scale: 0.0001\n",
    "    rgb_indices:\n",
    "      - 2\n",
    "      - 1\n",
    "      - 0\n",
    "    filter_indices:\n",
    "      - 2\n",
    "      - 1\n",
    "      - 0\n",
    "      - 3\n",
    "      - 4\n",
    "      - 5\n",
    "    train_data_root: <path to train data root>\n",
    "    val_data_root: <path to val data root>\n",
    "    test_data_root: <path to test data root>\n",
    "    img_grep: \"*_S2GeodnHand.tif\"\n",
    "    label_grep: \"*_LabelHand.tif\"\n",
    "    means:\n",
    "      - 0.107582\n",
    "      - 0.13471393\n",
    "      - 0.12520133\n",
    "      - 0.3236181\n",
    "      - 0.2341743\n",
    "      - 0.15878009\n",
    "    stds:\n",
    "      - 0.07145836\n",
    "      - 0.06783548\n",
    "      - 0.07323416\n",
    "      - 0.09489725\n",
    "      - 0.07938496\n",
    "      - 0.07089546\n",
    "    num_classes: 2\n",
    "\n",
    "model:\n",
    "  class_path: IBMSemanticSegmentationTask\n",
    "  init_args:\n",
    "    model_args:\n",
    "      decoder: FCNDecoder\n",
    "      pretrained: true\n",
    "      backbone: prithvi_vit_100\n",
    "      img_size: 512\n",
    "      decoder_channels: 256\n",
    "      in_channels: 6\n",
    "      bands:\n",
    "        - RED\n",
    "        - GREEN\n",
    "        - BLUE\n",
    "        - NIR_NARROW\n",
    "        - SWIR_1\n",
    "        - SWIR_2\n",
    "      num_frames: 1\n",
    "      num_classes: 2\n",
    "      head_dropout: 0.1\n",
    "      head_channel_list:\n",
    "        - 256\n",
    "    loss: ce\n",
    "    aux_heads:\n",
    "      - name: aux_head\n",
    "        decoder: FCNDecoder\n",
    "        decoder_args:\n",
    "          decoder_channels: 256\n",
    "          decoder_in_index: 2\n",
    "          decoder_num_convs: 1\n",
    "          head_channel_list:\n",
    "            - 64\n",
    "    aux_loss:\n",
    "      aux_head: 1.0\n",
    "    ignore_index: -1\n",
    "    class_weights:\n",
    "      - 0.3\n",
    "      - 0.7\n",
    "    freeze_backbone: false\n",
    "    freeze_decoder: false\n",
    "    model_factory: PrithviModelFactory\n",
    "optimizer:\n",
    "  class_path: torch.optim.AdamW\n",
    "  init_args:\n",
    "    lr: 6.e-5\n",
    "    weight_decay: 0.05\n",
    "lr_scheduler:\n",
    "  class_path: ReduceLROnPlateau\n",
    "  init_args:\n",
    "    monitor: val/loss\n",
    "```\n",
    "\n",
    "You can train with `terratorch fit --config <path_to_config_file>`\n",
    "\n",
    "You can test with `terratorch test --config <path_to_config_file> --ckpt_path <path_to_checkpoint_file>`\n",
    "\n",
    "You can run inference with \n",
    "```shell\n",
    "terratorch predict -c <path_to_config_file> --ckpt_path<path_to_checkpoint> --predict_output_dir <path_to_output_dir> --data.init_args.predict_data_root <path_to_input_dir> --data.init_args.predict_dataset_bands <all bands in the predicted dataset, e.g. [BLUE,GREEN,RED,NIR_NARROW,SWIR_1,SWIR_2,0]>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba9b09-f029-4559-acea-20614be4b788",
   "metadata": {},
   "source": [
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchgeo] *",
   "language": "python",
   "name": "conda-env-torchgeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
